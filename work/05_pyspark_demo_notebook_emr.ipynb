{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Demo Notebook\n",
    "## Demo\n",
    "1. Setup Spark\n",
    "2. Load Kaggle Data\n",
    "3. Transform Data with Spark SQL\n",
    "\n",
    "_Prepared by: [Gary A. Stafford](https://twitter.com/GaryStafford)   \n",
    "Associated article: https://wp.me/p1RD28-61V_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Spark\n",
    "Setup Spark SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Kaggle Data\n",
    "Load the Kaggle dataset from the CSV file, containing ~21K records, into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+-------------+\n",
      "|      date|    time|transaction|         item|\n",
      "+----------+--------+-----------+-------------+\n",
      "|2016-10-30|09:58:11|          1|        Bread|\n",
      "|2016-10-30|10:05:34|          2| Scandinavian|\n",
      "|2016-10-30|10:05:34|          2| Scandinavian|\n",
      "|2016-10-30|10:07:57|          3|Hot chocolate|\n",
      "|2016-10-30|10:07:57|          3|          Jam|\n",
      "|2016-10-30|10:07:57|          3|      Cookies|\n",
      "|2016-10-30|10:08:41|          4|       Muffin|\n",
      "|2016-10-30|10:13:03|          5|       Coffee|\n",
      "|2016-10-30|10:13:03|          5|       Pastry|\n",
      "|2016-10-30|10:13:03|          5|        Bread|\n",
      "+----------+--------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21293"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bakery_schema = StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"time\", StringType(), True),\n",
    "    StructField(\"transaction\", IntegerType(), True),\n",
    "    StructField(\"item\", StringType(), True)\n",
    "])\n",
    "df_bakery1 = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"s3://gstafford-aws-emr-notebooks/data/bakery-csv/BreadBasket_DMS.csv\", schema=bakery_schema)\n",
    "\n",
    "df_bakery1.show(10)\n",
    "df_bakery1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Data with Spark SQL\n",
    "Transform the DataFrame's bakery data using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame rows: 20507\n",
      "+----------+-----------+-------------+\n",
      "|date      |transaction|item         |\n",
      "+----------+-----------+-------------+\n",
      "|2016-10-30|1          |Bread        |\n",
      "|2016-10-30|2          |Scandinavian |\n",
      "|2016-10-30|2          |Scandinavian |\n",
      "|2016-10-30|3          |Hot chocolate|\n",
      "|2016-10-30|3          |Jam          |\n",
      "+----------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bakery1.createOrReplaceTempView(\"tmp_bakery\")\n",
    "df_bakery2 = spark.sql(\"SELECT date, transaction, item \" +\n",
    "                       \"FROM tmp_bakery \" +\n",
    "                       \"WHERE item NOT LIKE 'NONE'\" +\n",
    "                       \"ORDER BY transaction\")\n",
    "print(\"DataFrame rows: %d\" % df_bakery2.count())\n",
    "df_bakery2.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame rows: 98\n",
      "+----------+-----+\n",
      "|date      |count|\n",
      "+----------+-----+\n",
      "|2017-01-01|1    |\n",
      "|2017-01-03|87   |\n",
      "|2017-01-04|76   |\n",
      "|2017-01-05|95   |\n",
      "|2017-01-06|84   |\n",
      "+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bakery2.createOrReplaceTempView(\"tmp_bakery\")\n",
    "\n",
    "df_bakery3 = spark.sql(\"SELECT date, count(*) as count \" +\n",
    "                       \"FROM tmp_bakery \" +\n",
    "                       \"WHERE date >= '2017-01-01' \" +\n",
    "                       \"GROUP BY date \" +\n",
    "                       \"ORDER BY date\")\n",
    "print(\"DataFrame rows: %d\" % df_bakery3.count())\n",
    "df_bakery3.show(5, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

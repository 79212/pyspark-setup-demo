{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Demo Notebook\n",
    "## Set-up\n",
    "1. Download 'BreadBasket_DMS.csv'\n",
    "2. Run 'basic_scripts.sql'\n",
    "\n",
    "## Demo\n",
    "1. Load PostgreSQL Data\n",
    "2. Create New Record\n",
    "3. Write New Record to PostgreSQL Table\n",
    "4. Load CSV Data File\n",
    "5. Write Data to PostgreSQL\n",
    "6. Analyze Data with Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName('pyspark_demo_app')\\\n",
    "    .config('spark.driver.extraClassPath', \n",
    "            '/home/garystafford/work/postgresql-42.2.5.jar')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PostgreSQL Data\n",
    "Load the the PostgreSQL 'bakery_basket' table's 3 rows of data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = {\n",
    "    'driver': 'org.postgresql.Driver'\n",
    "}\n",
    "url = 'jdbc:postgresql://postgres:5432/demo'\n",
    "\n",
    "df1 = spark.read \\\n",
    "    .format('jdbc') \\\n",
    "    .option('url', url) \\\n",
    "    .option('user', 'postgres') \\\n",
    "    .option('password', 'postgres1234') \\\n",
    "    .option('driver', properties['driver']) \\\n",
    "    .option('dbtable', 'bakery_basket') \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------+-----------+-------------+\n| id|      date|    time|transaction|         item|\n+---+----------+--------+-----------+-------------+\n|  1|2016-10-30|09:58:11|          1|        Bread|\n|  2|2016-10-30|10:05:34|          2| Scandinavian|\n|  3|2016-10-30|10:07:57|          3|Hot chocolate|\n|  1|2016-10-30|10:13:27|          2|       Pastry|\n|  2|2016-10-30|09:58:11|          1|        Bread|\n|  3|2016-10-30|10:05:34|          2| Scandinavian|\n|  4|2016-10-30|10:05:34|          2| Scandinavian|\n|  5|2016-10-30|10:07:57|          3|Hot chocolate|\n|  6|2016-10-30|10:07:57|          3|          Jam|\n|  7|2016-10-30|10:07:57|          3|      Cookies|\n|  8|2016-10-30|10:08:41|          4|       Muffin|\n|  9|2016-10-30|10:13:03|          5|       Coffee|\n| 10|2016-10-30|10:13:03|          5|       Pastry|\n| 11|2016-10-30|10:13:03|          5|        Bread|\n| 12|2016-10-30|10:16:55|          6|    Medialuna|\n| 13|2016-10-30|10:16:55|          6|       Pastry|\n| 14|2016-10-30|10:16:55|          6|       Muffin|\n| 15|2016-10-30|10:19:12|          7|    Medialuna|\n| 16|2016-10-30|10:19:12|          7|       Pastry|\n| 17|2016-10-30|10:19:12|          7|       Coffee|\n+---+----------+--------+-----------+-------------+\nonly showing top 20 rows\n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63885"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.show()\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create New Record\n",
    "Create a new bakery record and load into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('2016-10-30', '10:13:27', 2, 'Pastry')]\n",
    "\n",
    "bakery_schema = StructType([\n",
    "    StructField('date', StringType(), True),\n",
    "    StructField('time', StringType(), True),\n",
    "    StructField('transaction', IntegerType(), True),\n",
    "    StructField('item', StringType(), True)\n",
    "])\n",
    "\n",
    "df2 = spark.createDataFrame(data, bakery_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+------+\n|      date|    time|transaction|  item|\n+----------+--------+-----------+------+\n|2016-10-30|10:13:27|          2|Pastry|\n+----------+--------+-----------+------+\n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.show()\n",
    "df2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write New Record to PostgreSQL Table\n",
    "Append the contents of the DataFrame to the PostgreSQL 'bakery_basket' table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write \\\n",
    "    .format('jdbc') \\\n",
    "    .option('url', url) \\\n",
    "    .option('user', 'postgres') \\\n",
    "    .option('password', 'postgres1234') \\\n",
    "    .option('driver', properties['driver']) \\\n",
    "    .option('dbtable', 'bakery_basket') \\\n",
    "    .mode('append') \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------+-----------+-------------+\n| id|      date|    time|transaction|         item|\n+---+----------+--------+-----------+-------------+\n|  1|2016-10-30|09:58:11|          1|        Bread|\n|  2|2016-10-30|10:05:34|          2| Scandinavian|\n|  3|2016-10-30|10:07:57|          3|Hot chocolate|\n|  1|2016-10-30|10:13:27|          2|       Pastry|\n|  2|2016-10-30|09:58:11|          1|        Bread|\n|  3|2016-10-30|10:05:34|          2| Scandinavian|\n|  4|2016-10-30|10:05:34|          2| Scandinavian|\n|  5|2016-10-30|10:07:57|          3|Hot chocolate|\n|  6|2016-10-30|10:07:57|          3|          Jam|\n|  7|2016-10-30|10:07:57|          3|      Cookies|\n|  8|2016-10-30|10:08:41|          4|       Muffin|\n|  9|2016-10-30|10:13:03|          5|       Coffee|\n| 10|2016-10-30|10:13:03|          5|       Pastry|\n| 11|2016-10-30|10:13:03|          5|        Bread|\n| 12|2016-10-30|10:16:55|          6|    Medialuna|\n| 13|2016-10-30|10:16:55|          6|       Pastry|\n| 14|2016-10-30|10:16:55|          6|       Muffin|\n| 15|2016-10-30|10:19:12|          7|    Medialuna|\n| 16|2016-10-30|10:19:12|          7|       Pastry|\n| 17|2016-10-30|10:19:12|          7|       Coffee|\n+---+----------+--------+-----------+-------------+\nonly showing top 20 rows\n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63886"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.show()\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSV File Data\n",
    "Load the Kaggle dataset from the CSV file, containing ~21K records, into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"BreadBasket_DMS.csv\", schema=bakery_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+-------------+\n|      date|    time|transaction|         item|\n+----------+--------+-----------+-------------+\n|2016-10-30|09:58:11|          1|        Bread|\n|2016-10-30|10:05:34|          2| Scandinavian|\n|2016-10-30|10:05:34|          2| Scandinavian|\n|2016-10-30|10:07:57|          3|Hot chocolate|\n|2016-10-30|10:07:57|          3|          Jam|\n|2016-10-30|10:07:57|          3|      Cookies|\n|2016-10-30|10:08:41|          4|       Muffin|\n|2016-10-30|10:13:03|          5|       Coffee|\n|2016-10-30|10:13:03|          5|       Pastry|\n|2016-10-30|10:13:03|          5|        Bread|\n+----------+--------+-----------+-------------+\nonly showing top 10 rows\n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21293"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.show(10)\n",
    "df3.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Data to PostgreSQL\n",
    "Append the contents of the DataFrame to the PostgreSQL 'bakery_basket' table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.write \\\n",
    "    .format('jdbc') \\\n",
    "    .option('url', url) \\\n",
    "    .option('user', 'postgres') \\\n",
    "    .option('password', 'postgres1234') \\\n",
    "    .option('driver', properties['driver']) \\\n",
    "    .option('dbtable', 'bakery_basket') \\\n",
    "    .mode('append') \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------+-----------+-------------+\n| id|      date|    time|transaction|         item|\n+---+----------+--------+-----------+-------------+\n|  1|2016-10-30|09:58:11|          1|        Bread|\n|  2|2016-10-30|10:05:34|          2| Scandinavian|\n|  3|2016-10-30|10:07:57|          3|Hot chocolate|\n|  1|2016-10-30|10:13:27|          2|       Pastry|\n|  2|2016-10-30|09:58:11|          1|        Bread|\n|  3|2016-10-30|10:05:34|          2| Scandinavian|\n|  4|2016-10-30|10:05:34|          2| Scandinavian|\n|  5|2016-10-30|10:07:57|          3|Hot chocolate|\n|  6|2016-10-30|10:07:57|          3|          Jam|\n|  7|2016-10-30|10:07:57|          3|      Cookies|\n+---+----------+--------+-----------+-------------+\nonly showing top 10 rows\n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "85179"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.show(10)\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Data with Spark SQL\n",
    "Analyze the DataFrame's bakery data using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------+-----------+------------+\n|   id|      date|    time|transaction|        item|\n+-----+----------+--------+-----------+------------+\n|    2|2016-10-30|09:58:11|          1|       Bread|\n|63884|2016-10-30|09:58:11|          1|       Bread|\n|21296|2016-10-30|09:58:11|          1|       Bread|\n|    1|2016-10-30|09:58:11|          1|       Bread|\n|42590|2016-10-30|09:58:11|          1|       Bread|\n|    2|2016-10-30|10:05:34|          2|Scandinavian|\n|    4|2016-10-30|10:05:34|          2|Scandinavian|\n|21298|2016-10-30|10:05:34|          2|Scandinavian|\n|42592|2016-10-30|10:05:34|          2|Scandinavian|\n|    3|2016-10-30|10:05:34|          2|Scandinavian|\n|21297|2016-10-30|10:05:34|          2|Scandinavian|\n|42591|2016-10-30|10:05:34|          2|Scandinavian|\n|63885|2016-10-30|10:05:34|          2|Scandinavian|\n|63886|2016-10-30|10:05:34|          2|Scandinavian|\n|21295|2016-10-30|10:13:27|          2|      Pastry|\n+-----+----------+--------+-----------+------------+\nonly showing top 15 rows\n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "85179"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.createOrReplaceTempView(\"bakery_table\")\n",
    "df4 = spark.sql(\"SELECT * FROM bakery_table \" +\n",
    "                \"ORDER BY transaction, date, time\")\n",
    "df4.show(15)\n",
    "df4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|item_count|\n+----------+\n|        95|\n+----------+\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n|         item|count|\n+-------------+-----+\n|       Coffee|21884|\n|        Bread|13301|\n|          Tea| 5740|\n|         Cake| 4100|\n|       Pastry| 3428|\n|     Sandwich| 3084|\n|    Medialuna| 2464|\n|Hot chocolate| 2361|\n|      Cookies| 2160|\n|      Brownie| 1516|\n+-------------+-----+\nonly showing top 10 rows\n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5 = spark.sql(\"SELECT COUNT(DISTINCT item) AS item_count FROM bakery_table\")\n",
    "df5.show()\n",
    "\n",
    "df5 = spark.sql(\"SELECT item, count(*) as count \" +\n",
    "                \"FROM bakery_table \" +\n",
    "                \"WHERE item NOT LIKE 'NONE' \" +\n",
    "                \"GROUP BY item ORDER BY count DESC\")\n",
    "df5.show(10)\n",
    "df5.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n|          timestamp|transaction|        item|\n+-------------------+-----------+------------+\n|2016-10-30 09:58:11|          1|       Bread|\n|2016-10-30 09:58:11|          1|       Bread|\n|2016-10-30 09:58:11|          1|       Bread|\n|2016-10-30 09:58:11|          1|       Bread|\n|2016-10-30 09:58:11|          1|       Bread|\n|2016-10-30 10:05:34|          2|Scandinavian|\n|2016-10-30 10:05:34|          2|Scandinavian|\n|2016-10-30 10:05:34|          2|Scandinavian|\n|2016-10-30 10:05:34|          2|Scandinavian|\n|2016-10-30 10:13:27|          2|      Pastry|\n+-------------------+-----------+------------+\nonly showing top 10 rows\n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "82035"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6 = spark.sql(\"SELECT CONCAT(date,' ',time) as timestamp, transaction, item \" +\n",
    "                \"FROM bakery_table \" +\n",
    "                \"WHERE item NOT LIKE 'NONE'\" +\n",
    "                \"ORDER BY transaction\"\n",
    "               )\n",
    "df6.show(10)\n",
    "df6.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- timestamp: timestamp (nullable = true)\n |-- transaction: integer (nullable = true)\n |-- item: string (nullable = true)\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n|          timestamp|transaction|        item|\n+-------------------+-----------+------------+\n|2016-10-30 09:58:11|          1|       Bread|\n|2016-10-30 09:58:11|          1|       Bread|\n|2016-10-30 09:58:11|          1|       Bread|\n|2016-10-30 09:58:11|          1|       Bread|\n|2016-10-30 09:58:11|          1|       Bread|\n|2016-10-30 10:05:34|          2|Scandinavian|\n|2016-10-30 10:05:34|          2|Scandinavian|\n|2016-10-30 10:05:34|          2|Scandinavian|\n|2016-10-30 10:05:34|          2|Scandinavian|\n|2016-10-30 10:13:27|          2|      Pastry|\n+-------------------+-----------+------------+\nonly showing top 10 rows\n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "82035"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7 = df6.withColumn('timestamp', to_timestamp(df6.timestamp, 'yyyy-MM-dd HH:mm:ss'))\n",
    "df7.printSchema()\n",
    "df7.show(10)\n",
    "df7.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+--------------+\n|          timestamp|transaction|          item|\n+-------------------+-----------+--------------+\n|2017-04-09 15:04:24|       9684|     Smoothies|\n|2017-04-09 14:57:06|       9683|        Pastry|\n|2017-04-09 14:57:06|       9683|        Coffee|\n|2017-04-09 14:32:58|       9682|           Tea|\n|2017-04-09 14:32:58|       9682|  Tacos/Fajita|\n|2017-04-09 14:32:58|       9682|        Muffin|\n|2017-04-09 14:32:58|       9682|        Coffee|\n|2017-04-09 14:30:09|       9681|           Tea|\n|2017-04-09 14:30:09|       9681|Spanish Brunch|\n|2017-04-09 14:30:09|       9681|      Truffles|\n+-------------------+-----------+--------------+\nonly showing top 10 rows\n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18888"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7.createOrReplaceTempView(\"bakery_table\")\n",
    "df8 = spark.sql(\"SELECT DISTINCT * \" +\n",
    "                \"FROM bakery_table \" +\n",
    "                \"WHERE item NOT LIKE 'NONE'\" +\n",
    "                \"ORDER BY transaction DESC\"\n",
    "                )\n",
    "df8.show(10)\n",
    "df8.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8.write.parquet('bakery_parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+---------+\n|          timestamp|transaction|     item|\n+-------------------+-----------+---------+\n|2017-02-15 14:54:25|       6620|     Cake|\n|2017-02-15 14:41:27|       6619|    Bread|\n|2017-02-15 14:40:41|       6618|   Coffee|\n|2017-02-15 14:40:41|       6618|    Bread|\n|2017-02-15 14:23:16|       6617| Baguette|\n|2017-02-15 14:23:16|       6617|   Coffee|\n|2017-02-15 14:23:16|       6617|    Salad|\n|2017-02-15 14:23:16|       6617| Art Tray|\n|2017-02-15 14:23:16|       6617|Alfajores|\n|2017-02-15 14:16:26|       6616|    Bread|\n+-------------------+-----------+---------+\nonly showing top 10 rows\n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18888"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df9 = spark.read.parquet('bakery_parquet')\n",
    "df9.show(10)\n",
    "df9.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

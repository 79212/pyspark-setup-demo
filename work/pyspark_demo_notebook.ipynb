{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Demo Notebook\n",
    "## Steps\n",
    "1. Deploy Docker stack\n",
    "2. Create PostgreSQL table and sample data using sql file\n",
    "2. Run Jupyter Notebook to test Docker stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrameReader\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName('pyspark_demo_app')\\\n",
    "    .config('spark.driver.extraClassPath', \n",
    "            '/home/garystafford/work/postgresql-42.2.5.jar')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Existing Data\n",
    "Read exising data from PostgreSQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = {\n",
    "    'driver': 'org.postgresql.Driver'\n",
    "}\n",
    "url = 'jdbc:postgresql://postgres:5432/demo'\n",
    "\n",
    "df1 = spark.read \\\n",
    "    .format('jdbc') \\\n",
    "    .option('url', url) \\\n",
    "    .option('user', 'postgres') \\\n",
    "    .option('password', 'postgres1234') \\\n",
    "    .option('driver', properties['driver']) \\\n",
    "    .option('dbtable', 'bakery_basket') \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------+-----------+-------------+\n",
      "| id|      date|    time|transaction|         item|\n",
      "+---+----------+--------+-----------+-------------+\n",
      "|  1|2016-10-30|09:58:11|          1|        Bread|\n",
      "|  2|2016-10-30|10:05:34|          2| Scandinavian|\n",
      "|  3|2016-10-30|10:07:57|          3|Hot chocolate|\n",
      "+---+----------+--------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Row to Table\n",
    "Add new row of data to PostgreSQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('2016-10-30', '10:13:27', 2, 'Pastry')]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('date', StringType(), True),\n",
    "    StructField('time', StringType(), True),\n",
    "    StructField('transaction', IntegerType(), True),\n",
    "    StructField('item', StringType(), True)\n",
    "])\n",
    "\n",
    "df2 = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write \\\n",
    "    .format('jdbc') \\\n",
    "    .option('url', url) \\\n",
    "    .option('user', 'postgres') \\\n",
    "    .option('password', 'postgres1234') \\\n",
    "    .option('driver', properties['driver']) \\\n",
    "    .option('dbtable', 'bakery_basket') \\\n",
    "    .mode('append') \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------+-----------+-------------+\n",
      "| id|      date|    time|transaction|         item|\n",
      "+---+----------+--------+-----------+-------------+\n",
      "|  1|2016-10-30|09:58:11|          1|        Bread|\n",
      "|  2|2016-10-30|10:05:34|          2| Scandinavian|\n",
      "|  3|2016-10-30|10:07:57|          3|Hot chocolate|\n",
      "|  1|2016-10-30|10:13:27|          2|       Pastry|\n",
      "+---+----------+--------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSV Data and Write to Database\n",
    "Read in Kaggle dataset from CSV file and append to existing PostgreSQL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_data = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .load(\"BreadBasket_DMS.csv\", schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+-------------+\n",
      "|      date|    time|transaction|         item|\n",
      "+----------+--------+-----------+-------------+\n",
      "|2016-10-30|09:58:11|          1|        Bread|\n",
      "|2016-10-30|10:05:34|          2| Scandinavian|\n",
      "|2016-10-30|10:05:34|          2| Scandinavian|\n",
      "|2016-10-30|10:07:57|          3|Hot chocolate|\n",
      "|2016-10-30|10:07:57|          3|          Jam|\n",
      "|2016-10-30|10:07:57|          3|      Cookies|\n",
      "|2016-10-30|10:08:41|          4|       Muffin|\n",
      "|2016-10-30|10:13:03|          5|       Coffee|\n",
      "|2016-10-30|10:13:03|          5|       Pastry|\n",
      "|2016-10-30|10:13:03|          5|        Bread|\n",
      "+----------+--------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kaggle_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_data.write \\\n",
    "    .format('jdbc') \\\n",
    "    .option('url', url) \\\n",
    "    .option('user', 'postgres') \\\n",
    "    .option('password', 'postgres1234') \\\n",
    "    .option('driver', properties['driver']) \\\n",
    "    .option('dbtable', 'bakery_basket') \\\n",
    "    .mode('append') \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------+-----------+-------------+\n",
      "| id|      date|    time|transaction|         item|\n",
      "+---+----------+--------+-----------+-------------+\n",
      "|  1|2016-10-30|09:58:11|          1|        Bread|\n",
      "|  2|2016-10-30|10:05:34|          2| Scandinavian|\n",
      "|  3|2016-10-30|10:07:57|          3|Hot chocolate|\n",
      "|  1|2016-10-30|10:13:27|          2|       Pastry|\n",
      "|  2|2016-10-30|09:58:11|          1|        Bread|\n",
      "|  3|2016-10-30|10:05:34|          2| Scandinavian|\n",
      "|  4|2016-10-30|10:05:34|          2| Scandinavian|\n",
      "|  5|2016-10-30|10:07:57|          3|Hot chocolate|\n",
      "|  6|2016-10-30|10:07:57|          3|          Jam|\n",
      "|  7|2016-10-30|10:07:57|          3|      Cookies|\n",
      "+---+----------+--------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Data with Spark SQL\n",
    "Analyze bakery data using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------+-----------+-------------+\n",
      "| id|      date|    time|transaction|         item|\n",
      "+---+----------+--------+-----------+-------------+\n",
      "|  1|2016-10-30|09:58:11|          1|        Bread|\n",
      "|  2|2016-10-30|09:58:11|          1|        Bread|\n",
      "|  3|2016-10-30|10:05:34|          2| Scandinavian|\n",
      "|  4|2016-10-30|10:05:34|          2| Scandinavian|\n",
      "|  2|2016-10-30|10:05:34|          2| Scandinavian|\n",
      "|  3|2016-10-30|10:07:57|          3|Hot chocolate|\n",
      "|  7|2016-10-30|10:07:57|          3|      Cookies|\n",
      "|  6|2016-10-30|10:07:57|          3|          Jam|\n",
      "|  5|2016-10-30|10:07:57|          3|Hot chocolate|\n",
      "|  8|2016-10-30|10:08:41|          4|       Muffin|\n",
      "| 11|2016-10-30|10:13:03|          5|        Bread|\n",
      "|  9|2016-10-30|10:13:03|          5|       Coffee|\n",
      "| 10|2016-10-30|10:13:03|          5|       Pastry|\n",
      "|  1|2016-10-30|10:13:27|          2|       Pastry|\n",
      "| 12|2016-10-30|10:16:55|          6|    Medialuna|\n",
      "+---+----------+--------+-----------+-------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.createOrReplaceTempView(\"bakery_table\")\n",
    "df1 = spark.sql(\"SELECT * FROM bakery_table ORDER BY date, time\")\n",
    "df1.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|         item|count|\n",
      "+-------------+-----+\n",
      "|       Coffee| 5471|\n",
      "|        Bread| 3326|\n",
      "|          Tea| 1435|\n",
      "|         Cake| 1025|\n",
      "|       Pastry|  857|\n",
      "|         NONE|  786|\n",
      "|     Sandwich|  771|\n",
      "|    Medialuna|  616|\n",
      "|Hot chocolate|  591|\n",
      "|      Cookies|  540|\n",
      "+-------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.sql(\"SELECT item, count(*) as count FROM bakery_table GROUP BY item ORDER BY count DESC\")\n",
    "df1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
